name: Data Crawl and Process

permissions:
  contents: write  # üëà Cho ph√©p commit v√† push code

on:
  push:
    branches:
      - main
  schedule:
    - cron: '0 2 * * *'  # Ch·∫°y m·ªói ng√†y l√∫c 2h s√°ng

jobs:
  crawl-and-process:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd VietNewsSummerize-crawl_17-05
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
           pip install -r requirements.txt
          fi
      - name: Run scraper script
        run: |
          cd VietNewsSummerize-crawl_17-05
          python Crawler/Sample/Scraper/main.py

      - name: Commit and push updated data
        run: |
          cd VietNewsSummerize-crawl_17-05
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git pull origin main
          git add Crawler/Data/
          git commit -m "Auto update crawled data" || echo "No changes to commit"
          git push origin main
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
